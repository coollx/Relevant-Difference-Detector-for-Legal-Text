{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer  # Twitter-aware tokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_tweet(text):\n",
    "    \"\"\"Returns a normalized versions of text.\"\"\"\n",
    "\n",
    "    # change hyperlinks to '<url>' tokens\n",
    "    output = re.sub(r'http[s]{0,1}://t.co/[a-zA-Z0-9]+\\b', '<url>', text)\n",
    "    \n",
    "    # separate all '#' signs from following word with one whitespace\n",
    "    output = re.sub(r'#(\\w+)', r'# \\1', output)\n",
    "\n",
    "    return output\n",
    "\n",
    "def _tokenize(tokenizer, string):\n",
    "    \"\"\"Tokenizes a sentence, but leave hastags (#) and users (@)\"\"\"\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(string)\n",
    "    return tokenized\n",
    "\n",
    "def _numbers_to_number_tokens(tokenized_string, num_token='<number>'):\n",
    "    \"\"\"Returns the tokenized string (list) with numbers replaced by a numbet token.\"\"\"\n",
    "    \n",
    "    # create a list of (word, POS-tags) tuples\n",
    "    pos_tagged = nltk.pos_tag(tokenized_string)\n",
    "    \n",
    "    # find indices of number POS tags\n",
    "    num_indices = [idx for idx in range(len(pos_tagged)) if pos_tagged[idx][1] == 'CD']\n",
    "    \n",
    "    # replace numbers by token\n",
    "    for idx in num_indices:\n",
    "        tokenized_string[idx] = num_token\n",
    "        \n",
    "    return tokenized_string  \n",
    "\n",
    "def preprocess_text(tokenizer, string):\n",
    "    \"\"\"Executes all text cleaning functions.\"\"\"\n",
    "    \n",
    "    return _numbers_to_number_tokens(_tokenize(tokenizer, _normalize_tweet(string)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('legal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06852cdbcf28824f4856c911260dff4aa3407a086ba977abbb1a931f2b398117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get article mapping to achieve article alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "df = pd.read_excel('data/category_configuration_09-08-2022_08-08-01.xlsx', sheet_name = 'article_names_matching')\n",
    "title2category = dict(zip(df[\"Article Title\"], df[\"Category 2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help functions to get article alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(article):\n",
    "    text = \"\"\n",
    "    for child in article:\n",
    "        text += \" \"+child.text\n",
    "    return text\n",
    "\n",
    "def article_alignment(doc1_path, doc2_path):\n",
    "    doc1, doc2 = ET.parse(doc1_path), ET.parse(doc2_path)\n",
    "    root1, root2 = doc1.getroot(), doc2.getroot()\n",
    "    try:\n",
    "        body1, body2 = root1[1][2], root2[1][2]\n",
    "    except:\n",
    "        return\n",
    "\n",
    "    #model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
    "    article_dict = {}\n",
    "\n",
    "    #if both articles attribute includes title\n",
    "    if \"title\" in body1[0].attrib and \"title\" in body2[0].attrib:\n",
    "        for article1 in body1:\n",
    "            title1 = article1.get(\"title\").lower()\n",
    "            for article2 in body2:\n",
    "                title2 = article2.get(\"title\").lower()\n",
    "\n",
    "                if title2category[title1] == title2category[title2]:\n",
    "                    #to make sure that they have a high similarity\n",
    "                    scores = util.cos_sim(\n",
    "                        model.encode(get_article_text(article1)), \n",
    "                        model.encode(get_article_text(article2))\n",
    "                    )\n",
    "                    if max(scores[0])>0.7:\n",
    "                        article_dict[article1.get(\"num\")] = article2.get(\"num\")\n",
    "\n",
    "    else:\n",
    "    #title not included in article attri, use sentence similarity instead\n",
    "        for article1 in body1:\n",
    "\n",
    "            text1_embd = model.encode(get_article_text(article1))\n",
    "            text2_list_embd = model.encode([get_article_text(article2) for article2 in body2])\n",
    "            scores = util.cos_sim(text1_embd, text2_list_embd)\n",
    "            maxi = max(scores[0]).item()\n",
    "            print(maxi)\n",
    "\n",
    "            if maxi > 0.7:\n",
    "                index = np.argmax(scores[0])\n",
    "                article_dict[article1.get(\"num\")] = body2[index].get(\"num\")\n",
    "\n",
    "\n",
    "\n",
    "    return article_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract similar sentences from two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_similar(doc1_path, doc2_path):\n",
    "    ret = []\n",
    "\n",
    "    #First get article alignments insides two documents\n",
    "    article_dict = article_alignment(doc1_path, doc2_path)\n",
    "    doc1, doc2 = ET.parse(doc1_path), ET.parse(doc2_path)\n",
    "    root1, root2 = doc1.getroot(), doc2.getroot()\n",
    "    try:\n",
    "        body1, body2 = root1[1][2], root2[1][2]\n",
    "    except:\n",
    "        return\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    for article1 in body1:\n",
    "        article2_num = article_dict.get(article1.get(\"num\"))\n",
    "        if article2_num:\n",
    "        #if current article have alignment in anthoer document\n",
    "\n",
    "            # print(\"Current document match: \")\n",
    "            # print(article1.get('num') + '   :   ' + article2_num)\n",
    "            article2 = body2.find(\".//div[@num='\" + article2_num + \"']\")\n",
    "            article2_sents = []\n",
    "\n",
    "            for child in article2:\n",
    "                article2_sents.append(child.text)\n",
    "\n",
    "            sentences2_embd = model.encode(article2_sents)\n",
    "\n",
    "            for sentence1 in article1:\n",
    "                sentence1_embd = model.encode(sentence1.text)\n",
    "                scores = util.cos_sim(sentence1_embd, sentences2_embd)\n",
    "                maxi = max(scores[0]).item()\n",
    "\n",
    "                if 0.5 < maxi < 0.95:\n",
    "                    index = np.argmax(scores[0])\n",
    "                    ret.append((sentence1.text, article2[index].text, maxi))\n",
    "                    # print(sentence1.text)\n",
    "                    # print(maxi)\n",
    "                    # print(article2[index].text)\n",
    "                    # print(\"----------------------------------------------------\")\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8434820175170898\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time() \n",
    "diff = extract_similar('data/full data/t1989-9-canada-russian-federation-bit-1989.xml', 'data/full data/t1990-14-canada-czech-republic-bit-1990.xml')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重构了亿小下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml2sentences(article, break_sentence = True):\n",
    "    '''\n",
    "    Break article into sentences. Break sentences with \".\"\n",
    "    article: xml element\n",
    "    '''\n",
    "    ret = []\n",
    "    for child in article:\n",
    "        if break_sentence:\n",
    "            ret.extend(child.text.strip().split(\".\"))\n",
    "        else:\n",
    "            ret.append(child.text)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_article_alignment(article_body1, article_body2, title2category, sanity_check = True):\n",
    "    '''\n",
    "    Get article alignment between two documents\n",
    "    article_body: xml element\n",
    "    title2category: dictionary of title to category\n",
    "    sanity_check: use similarity score to check if the alignment is correct\n",
    "    return a dictionary of article alignment\n",
    "    '''\n",
    "    alignment_match = {}\n",
    "\n",
    "    #if both articles attribute includes title\n",
    "    if 'title' in article_body1[0].attrib and 'title' in article_body2[0].attrib:\n",
    "        article_title1 = [(article.get('title').lower(), article.get('num')) for article in article_body1]\n",
    "        article_title2 = [(article.get('title').lower(), article.get('num')) for article in article_body2]\n",
    "        for index1, (title1, num1) in enumerate(article_title1):\n",
    "            for index2, (title2, num2) in enumerate(article_title2):\n",
    "                if title2category[title1] == title2category[title2]:\n",
    "                    #to make sure that they have a high similarity\n",
    "                    if sanity_check and max(util.cos_sim(\n",
    "                        model.encode(''.join(xml2sentences(article_body1[index1]))),\n",
    "                        model.encode(''.join(xml2sentences(article_body2[index2])))\n",
    "                    )).item() < 0.7:\n",
    "                        continue\n",
    "                    alignment_match[num1] = num2\n",
    "                    break\n",
    "                \n",
    "    #title not included in article attri, use sentence similarity instead\n",
    "    else:\n",
    "        for article1 in article_body1:\n",
    "            text1_embd = model.encode(''.join(xml2sentences(article1)))\n",
    "            text2_list_embd = model.encode([''.join(xml2sentences(article2)) for article2 in article_body2])\n",
    "            scores = util.cos_sim(text1_embd, text2_list_embd)\n",
    "            if max(scores[0]).item() > 0.7:\n",
    "                index = np.argmax(scores[0])\n",
    "                alignment_match[article1.get('num')] = article_body2[index].get('num')\n",
    "\n",
    "    return alignment_match\n",
    "    \n",
    "\n",
    "def extract_similar_sentences_from_article(article1, article2):\n",
    "    '''\n",
    "    article1, article2: xml element\n",
    "    return: list of similar sentences: (sentence1, sentence2, similarity)\n",
    "    '''\n",
    "    ret = []\n",
    "    article1_sents, article2_sents = xml2sentences(article1, break_sentence = False), xml2sentences(article2, break_sentence = False)\n",
    "    # article1_sents, article2_sents = xml2sentences(article1), xml2sentences(article2)\n",
    "\n",
    "    #Embed article1 and article2\n",
    "    article1_embd, article2_embd = model.encode(article1_sents), model.encode(article2_sents)\n",
    "\n",
    "    #Get similarity between article1 and article2\n",
    "    scores = util.cos_sim(article1_embd, article2_embd)\n",
    "\n",
    "    visited = set() #to make sure that we don't add the same sentence twice\n",
    "\n",
    "    #filter out the sentence with similarity greater than 0.98, this means they are perfect match and no need to compare\n",
    "    identical = (scores > 0.98).to(torch.int64)\n",
    "    for i, j in identical.nonzero().tolist():\n",
    "        visited.add('row' + str(i))\n",
    "        visited.add('col' + str(j))\n",
    "\n",
    "    #filter out the sentences with similarity between 0.5 and 0.95\n",
    "    mask = (scores > 0.5) & (scores < 0.95)\n",
    "    scores *= mask.to(torch.int64) \n",
    "\n",
    "    #get the index of the sentences with similarity between 0.5 and 0.95\n",
    "    sim_pairs = [(scores[i][j], i, j) for i, j in mask.nonzero().tolist()]\n",
    "    sim_pairs.sort(key = lambda x: x[0]) #sort by similarity score\n",
    "    \n",
    "    while sim_pairs:\n",
    "        score, i, j = sim_pairs.pop()\n",
    "        if 'row' + str(i) not in visited and 'col' + str(j) not in visited:\n",
    "            ret.append((article1_sents[i], article2_sents[j], scores[i][j].item()))\n",
    "            visited.add('row' + str(i))\n",
    "            visited.add('col' + str(j))\n",
    "\n",
    "    return ret\n",
    "\n",
    "def extract_similar_from_doc(doc1_path, doc2_path, title2category):\n",
    "    '''\n",
    "    doc_path: path to first document\n",
    "    title2category: dictionary of title to category\n",
    "    '''\n",
    "\n",
    "    try:   \n",
    "        doc1, doc2 = ET.parse(doc1_path), ET.parse(doc2_path)\n",
    "        doc_root1, doc_root2 = doc1.getroot(), doc2.getroot()\n",
    "        doc_body1, doc_body2 = doc_root1[1][2], doc_root2[1][2]\n",
    "    except Exception as e:\n",
    "        print(e); return []\n",
    "    \n",
    "    #get article alignment between two documents\n",
    "    alignment_match = get_article_alignment(doc_body1, doc_body2, title2category)\n",
    "\n",
    "    ret = []\n",
    "    for page1, page2 in alignment_match.items():\n",
    "        article1 = doc_body1.find(\".//div[@num='\" + page1 + \"']\")\n",
    "        article2 = doc_body2.find(\".//div[@num='\" + page2 + \"']\")\n",
    "        ret.extend(extract_similar_sentences_from_article(article1, article2))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40625\n"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "# meta_data_df = pd.read_excel('data/category_configuration_09-08-2022_08-08-01.xlsx', sheet_name = 'article_names_matching')\n",
    "# title2category = dict(zip(meta_data_df[\"Article Title\"], meta_data_df[\"Category 2\"]))\n",
    "start = time.time()\n",
    "diff = extract_similar_from_doc(\n",
    "    'data/full data/t1989-9-canada-russian-federation-bit-1989.xml', \n",
    "    'data/full data/t1990-14-canada-czech-republic-bit-1990.xml',\n",
    "    title2category\n",
    ")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('legal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06852cdbcf28824f4856c911260dff4aa3407a086ba977abbb1a931f2b398117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

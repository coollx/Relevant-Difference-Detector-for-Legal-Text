{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import xlwt\n",
    "\n",
    "from utils import elem2sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and metadata from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "meta_data_df = pd.read_excel('data/category_configuration_09-08-2022_08-08-01.xlsx', sheet_name = 'article_names_matching')\n",
    "title2category = dict(zip(meta_data_df[\"Article Title\"], meta_data_df[\"Category 2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for calculate similarity between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_alignment(all_articles1, all_articles2, title2category, sanity_check = True):\n",
    "    '''\n",
    "    Get article alignment between two documents\n",
    "    article_body: xml element\n",
    "    title2category: dictionary of title to category\n",
    "    sanity_check: use similarity score to check if the alignment is correct\n",
    "    return a dictionary of article alignment\n",
    "    '''\n",
    "    alignment_match = {}\n",
    "\n",
    "    #if both articles attribute includes title\n",
    "    if 'title' in all_articles1[0].attrib and 'title' in all_articles2[0].attrib:\n",
    "        article_title1 = [(article.get('title').lower(), article.get('num')) for article in all_articles1 if article.get('title')]\n",
    "        article_title2 = [(article.get('title').lower(), article.get('num')) for article in all_articles2 if article.get('title')]\n",
    "        for index1, (title1, num1) in enumerate(article_title1):\n",
    "            for index2, (title2, num2) in enumerate(article_title2):\n",
    "                if title2category.get(title1) == title2category.get(title2):\n",
    "                    #to make sure that they have a high similarity\n",
    "                    if sanity_check and max(util.cos_sim(\n",
    "                        model.encode(''.join(elem2sent(all_articles1[index1]))),\n",
    "                        model.encode(''.join(elem2sent(all_articles2[index2])))\n",
    "                    )).item() < 0.7:\n",
    "                        continue\n",
    "                    alignment_match[num1] = num2\n",
    "                    break\n",
    "                \n",
    "    #title not included in article attri, use sentence similarity instead\n",
    "    else:\n",
    "        for article1 in all_articles1:\n",
    "            text1_embd = model.encode(''.join(elem2sent(article1)))\n",
    "            text2_list_embd = model.encode([''.join(elem2sent(article2)) for article2 in all_articles2])\n",
    "            scores = util.cos_sim(text1_embd, text2_list_embd)\n",
    "            if max(scores[0]).item() > 0.7:\n",
    "                index = np.argmax(scores[0])\n",
    "                alignment_match[article1.get('num')] = all_articles2[index].get('num')\n",
    "\n",
    "    return alignment_match\n",
    "    \n",
    "\n",
    "def extract_similar_sentences_from_article(article1, article2):\n",
    "    '''\n",
    "    article1, article2: xml element\n",
    "    return: list of similar sentences: (sentence1, sentence2, similarity)\n",
    "    '''\n",
    "    article1_sents, article2_sents = elem2sent(article1, break_sentence = False), elem2sent(article2, break_sentence = False)\n",
    "    #article1_sents, article2_sents = elem2sent(article1), elem2sent(article2)\n",
    "\n",
    "    #Embed article1 and article2\n",
    "    article1_embd, article2_embd = model.encode(article1_sents), model.encode(article2_sents)\n",
    "\n",
    "    #Get similarity between article1 and article2\n",
    "    scores = util.cos_sim(article1_embd, article2_embd)\n",
    "\n",
    "    visited = set() #to make sure that we don't add the same sentence twice\n",
    "\n",
    "    #filter out the sentence with similarity greater than 0.98, this means they are perfect match and no need to compare\n",
    "    identical = (scores > 0.98).to(torch.int64)\n",
    "    for i, j in identical.nonzero().tolist():\n",
    "        visited.add('row' + str(i))\n",
    "        visited.add('col' + str(j))\n",
    "\n",
    "    #filter out the sentences with similarity between 0.5 and 0.98\n",
    "    mask = (scores > 0.5) & (scores < 0.98)\n",
    "    scores *= mask.to(torch.int64) \n",
    "\n",
    "    #get the index of the sentences with similarity between 0.5 and 0.98\n",
    "    sim_pairs = [(scores[i][j], i, j) for i, j in mask.nonzero().tolist()]\n",
    "    sim_pairs.sort(key = lambda x: x[0]) #sort by similarity score\n",
    "    \n",
    "    ret = []\n",
    "    while sim_pairs:\n",
    "        score, i, j = sim_pairs.pop()\n",
    "        if 'row' + str(i) not in visited and 'col' + str(j) not in visited:\n",
    "            ret.append((article1_sents[i], article2_sents[j], scores[i][j].item()))\n",
    "            visited.add('row' + str(i))\n",
    "            visited.add('col' + str(j))\n",
    "\n",
    "    return ret\n",
    "\n",
    "def extract_similar_from_doc(doc1_path, doc2_path, title2category, min_length = 5):\n",
    "    '''\n",
    "    doc_path: path to first document\n",
    "    title2category: dictionary of title to category\n",
    "    '''\n",
    "\n",
    "    try:   \n",
    "        doc1, doc2 = ET.parse(doc1_path), ET.parse(doc2_path)\n",
    "        doc_root1, doc_root2 = doc1.getroot(), doc2.getroot()\n",
    "        doc_body1, doc_body2 = doc_root1[1][2], doc_root2[1][2]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e); return []\n",
    "    \n",
    "    #get article alignment between two documents\n",
    "    all_articles1, all_articles2 = doc_body1.findall(\".//div[@type='article']\"), doc_body2.findall(\".//div[@type='article']\")\n",
    "    #alignment_match = get_article_alignment(doc_body1, doc_body2, title2category)\n",
    "    alignment_match = get_article_alignment(all_articles1, all_articles2, title2category)\n",
    "\n",
    "\n",
    "    ret = []\n",
    "    for page1, page2 in alignment_match.items():\n",
    "        article1, article2 = doc_body1.find(\".//div[@num='\" + page1 + \"']\"), doc_body2.find(\".//div[@num='\" + page2 + \"']\") \n",
    "        try:\n",
    "            if \"Fair and Equitable Treatment\" in [title2category.get(alt.get(\"title\").lower()) for alt in [article1,article2] ]:#check for None\n",
    "                ret.extend(extract_similar_sentences_from_article(article1, article2))\n",
    "        except:\n",
    "            continue\n",
    "    #filter out the pairs in which both sentences are longer than min_length words and length difference is less than 4 * min_length\n",
    "    ret = [x for x in ret if len(x[0].split()) > min_length and len(x[1].split()) > min_length and abs(len(x[0].split()) - len(x[1].split())) < 4 * min_length]\n",
    "\n",
    "    #sort by similarity score\n",
    "    ret.sort(key = lambda x: x[2], reverse = True)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanaity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1650044918060303\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "diff = extract_similar_from_doc(\n",
    "    'data/full data/t1989-9-canada-russian-federation-bit-1989.xml', \n",
    "    'data/full data/t1990-14-canada-czech-republic-bit-1990.xml',\n",
    "    title2category\n",
    ")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write all the sentences in a excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total treaty is 3309\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "docs = ['data/full data/' + _ for _ in os.listdir('data/full data') ]\n",
    "print(\"total treaty is \" + str(len(docs)))\n",
    "target_treaty = []\n",
    "\n",
    "for treaty in docs:\n",
    "    try:   \n",
    "        doc1 = ET.parse(treaty)\n",
    "        doc_root1 = doc1.getroot()\n",
    "        doc_body1 = doc_root1[1][2]\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    for article in doc_body1:\n",
    "\n",
    "        try:\n",
    "            if title2category.get(article.get(\"title\").lower()) == \"Fair and Equitable Treatment\":\n",
    "                target_treaty.append(treaty)\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(len(target_treaty))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_docs = target_treaty\n",
    "\n",
    "#Write all sentences to a excel file\n",
    "wb = xlwt.Workbook()\n",
    "ws = wb.add_sheet('sheet1')\n",
    "\n",
    "#add header\n",
    "ws.write(0, 0, 'sentence1')\n",
    "ws.write(0, 1, 'sentence2')\n",
    "ws.write(0, 2, 'similarity')\n",
    "ws.write(0, 3, 'doc1')\n",
    "ws.write(0, 4, 'doc2')\n",
    "\n",
    "row = 1\n",
    "\n",
    "for i in range(len(canada_docs)):\n",
    "    doc1 = canada_docs[i]\n",
    "    doc1name = doc1.replace('.','/').split('/')[2]\n",
    "\n",
    "    for j in range(3):\n",
    "        doc2 = random.choice(canada_docs)\n",
    "        doc2name = doc2.replace('.','/').split('/')[2]\n",
    "\n",
    "        try:\n",
    "            _ = extract_similar_from_doc(doc1, doc2, title2category)\n",
    "            for j in range(len(_)):\n",
    "                ws.write(row, 0, _[j][0])\n",
    "                ws.write(row, 1, _[j][1])\n",
    "                ws.write(row, 2, _[j][2])\n",
    "                ws.write(row, 3, doc1name)\n",
    "                ws.write(row, 4, doc2name)\n",
    "\n",
    "                row += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(doc1, doc2)\n",
    "            continue\n",
    "\n",
    "wb.save('generated_data/similar_sentences.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('legal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06852cdbcf28824f4856c911260dff4aa3407a086ba977abbb1a931f2b398117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

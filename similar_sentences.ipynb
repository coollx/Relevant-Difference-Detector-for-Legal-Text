{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import xlwt\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and metadata from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Xiang/.cache\\torch\\sentence_transformers\\nlpaueb_legal-bert-base-uncased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\Xiang/.cache\\torch\\sentence_transformers\\nlpaueb_legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('nlpaueb/legal-bert-base-uncased')\n",
    "#model = SentenceTransformer('nlpaueb/bert-base-uncased-contracts')\n",
    "meta_data_df = pd.read_excel('data/category_configuration_09-08-2022_08-08-01.xlsx', sheet_name = 'article_names_matching')\n",
    "title2category = dict(zip(meta_data_df[\"Article Title\"], meta_data_df[\"Category 2\"]))\n",
    "\n",
    "keyword_df = pd.read_excel('data/category_configuration_09-08-2022_08-08-01.xlsx', sheet_name = 'keywords_category_2_mapping')\n",
    "#keyword2category2 = dict(zip(keyword_df[\"Keyword\"], keyword_df[\"Category 2\"]))\n",
    "#keyword2category3 = dict(zip(keyword_df[\"Keyword\"], keyword_df[\"Category 3\"]))\n",
    "keyword_df.dropna(subset = [\"Keyword\"], inplace=True)\n",
    "keyword2category = {}\n",
    "for i in range(len(keyword_df)):\n",
    "    cat2 = keyword_df.iloc[i]['Category 2']\n",
    "    cat3 = keyword_df.iloc[i]['Category 3']\n",
    "    keyword = keyword_df.iloc[i]['Keyword']\n",
    "    if keyword not in keyword2category:\n",
    "        keyword2category[keyword] = {}\n",
    "    if cat2 not in keyword2category[keyword]:\n",
    "        keyword2category[keyword][cat2] = set()\n",
    "    keyword2category[keyword][cat2].add(cat3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for calculate similarity between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_alignment(all_articles1, all_articles2, title2category, sanity_check = True):\n",
    "    '''\n",
    "    Get article alignment between two documents\n",
    "    article_body: xml element\n",
    "    title2category: dictionary of title to category\n",
    "    sanity_check: use similarity score to check if the alignment is correct\n",
    "    return a dictionary of article alignment\n",
    "    '''\n",
    "    alignment_match = {}\n",
    "\n",
    "    #filter out the arrticles that don't have title\n",
    "    all_articles1 = [article for article in all_articles1 if article.get(\"title\") is not None]\n",
    "    all_articles2 = [article for article in all_articles2 if article.get(\"title\") is not None]\n",
    "\n",
    "    #if both articles attribute includes title\n",
    "    #if 'title' in all_articles1[0].attrib and 'title' in all_articles2[0].attrib:\n",
    "    article_title1 = [(article.get('title').lower(), article.get('num')) for article in all_articles1 if article.get('title')]\n",
    "    article_title2 = [(article.get('title').lower(), article.get('num')) for article in all_articles2 if article.get('title')]\n",
    "    for index1, (title1, num1) in enumerate(article_title1):\n",
    "        for index2, (title2, num2) in enumerate(article_title2):\n",
    "            if title2category.get(title1) == title2category.get(title2):\n",
    "                #to make sure that they have a high similarity\n",
    "                if sanity_check and max(util.cos_sim(\n",
    "                    model.encode(''.join(elem2sent(all_articles1[index1]))),\n",
    "                    model.encode(''.join(elem2sent(all_articles2[index2])))\n",
    "                )).item() < 0.7:\n",
    "                    continue\n",
    "                alignment_match[num1] = num2\n",
    "                break\n",
    "            \n",
    "    return alignment_match\n",
    "    \n",
    "\n",
    "def extract_similar_sentences_from_article(article1, article2):\n",
    "    '''\n",
    "    article1, article2: xml element\n",
    "    return: list of similar sentences and their category: (sentence1, sentence2, similarity), category\n",
    "    '''\n",
    "    if article1.get('title') and article2.get('title'):\n",
    "        if (title2category.get(article1.get('title').lower()) != title2category.get(article2.get('title').lower())):\n",
    "            return [], 'TITLE_MISMATCH'\n",
    "\n",
    "    article1_sents, article2_sents = elem2sent(article1, break_sentence = True), elem2sent(article2, break_sentence = False)\n",
    "    #article1_sents, article2_sents = elem2sent(article1), elem2sent(article2)\n",
    "\n",
    "    #Embed article1 and article2\n",
    "    article1_embd, article2_embd = model.encode(article1_sents), model.encode(article2_sents)\n",
    "\n",
    "    #Get similarity between article1 and article2\n",
    "    scores = util.cos_sim(article1_embd, article2_embd)\n",
    "\n",
    "    visited = set() #to make sure that we don't add the same sentence twice\n",
    "\n",
    "    #filter out the sentence with similarity greater than 0.98, this means they are perfect match and no need to compare\n",
    "    identical = (scores > 0.999).to(torch.int64)\n",
    "    for i, j in identical.nonzero().tolist():\n",
    "        visited.add('row' + str(i))\n",
    "        visited.add('col' + str(j))\n",
    "\n",
    "    #filter out the sentences with similarity between 0.5 and 0.98\n",
    "    mask = (scores >= 0.8) & (scores < 1)\n",
    "    scores *= mask.to(torch.int64) \n",
    "\n",
    "    #get the index of the sentences with similarity between 0.5 and 0.98\n",
    "    sim_pairs = [(scores[i][j], i, j) for i, j in mask.nonzero().tolist()]\n",
    "    sim_pairs.sort(key = lambda x: x[0]) #sort by similarity score\n",
    "    \n",
    "    ret = []\n",
    "    while sim_pairs:\n",
    "        score, i, j = sim_pairs.pop()\n",
    "        if 'row' + str(i) not in visited and 'col' + str(j) not in visited:\n",
    "            ret.append((article1_sents[i], article2_sents[j], scores[i][j].item()))\n",
    "            visited.add('row' + str(i))\n",
    "            visited.add('col' + str(j))\n",
    "\n",
    "    return ret, title2category.get(article1.get('title').lower())\n",
    "\n",
    "def get_cat3(sentence, keyword2category, cat2):\n",
    "    '''\n",
    "    sentence: string\n",
    "    keyword2category: dictionary of keyword to category\n",
    "    cat2: string\n",
    "    return: category 3\n",
    "    '''\n",
    "    kws = set()\n",
    "    res = set()\n",
    "    sentence = sentence.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    for keyword in keyword2category.keys():\n",
    "        if keyword in sentence and cat2 in keyword2category[keyword]:\n",
    "            kws.add(keyword)\n",
    "            res |= keyword2category[keyword][cat2]\n",
    "            #return keyword, keyword2category[keyword].get(cat2, set())\n",
    "    return kws, res\n",
    "\n",
    "def extract_similar_from_doc(doc1_path, doc2_path, title2category, exclude_category = [], min_length = 5):\n",
    "    '''\n",
    "    doc_path: path to first document\n",
    "    title2category: dictionary of title to category\n",
    "    '''\n",
    "\n",
    "    try:   \n",
    "        doc1, doc2 = ET.parse(doc1_path), ET.parse(doc2_path)\n",
    "        doc_root1, doc_root2 = doc1.getroot(), doc2.getroot()\n",
    "        doc_body1, doc_body2 = doc_root1[1][2], doc_root2[1][2]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e); return []\n",
    "    \n",
    "    #get article alignment between two documents\n",
    "    all_articles1, all_articles2 = doc_body1.findall(\".//div[@type='article']\"), doc_body2.findall(\".//div[@type='article']\")\n",
    "    alignment_match = get_article_alignment(all_articles1, all_articles2, title2category)\n",
    "\n",
    "    ret = []\n",
    "    for page1, page2 in alignment_match.items():\n",
    "        if not page1 or not page2: continue\n",
    "        article1, article2 = doc_body1.find(\".//div[@num='\" + page1 + \"']\"), doc_body2.find(\".//div[@num='\" + page2 + \"']\") \n",
    "        similar_sents, category2 = extract_similar_sentences_from_article(article1, article2)\n",
    "        \n",
    "        if category2 == 'TITLE_MISMATCH' or category2 in exclude_category:\n",
    "            continue\n",
    "        #print(category2)\n",
    "\n",
    "        for (sentence1, sentence2, score) in similar_sents:\n",
    "\n",
    "            k1, s1_cate3 = get_cat3(sentence1, keyword2category, category2)\n",
    "            k2, s2_cate3 = get_cat3(sentence2, keyword2category, category2)\n",
    "\n",
    "\n",
    "            #filter out the pairs which has a category and which both sentences are longer than min_length words and length difference is less than 4 * min_length\n",
    "            if (s1_cate3 and s2_cate3 and len(sentence1.split()) >= min_length and len(sentence2.split()) >= min_length and abs(len(sentence1) - len(sentence2)) <= 4 * min_length):\n",
    "                # if s1_cate3 == s2_cate3 or s1_cate3.issubset(s2_cate3) or s2_cate3.issubset(s1_cate3):\n",
    "                if s1_cate3 == s2_cate3:\n",
    "                    if 0.965 < score < 0.98:\n",
    "                        ret.append(('STYLYSTIC', sentence1, sentence2, score, list(s1_cate3)))\n",
    "                elif s1_cate3 != s2_cate3 and iou(s1_cate3, s2_cate3) < 1/3 and score > 0.9:\n",
    "                    ret.append(('RELEVANT', sentence1, sentence2, score, [list(s1_cate3), list(s2_cate3)]))\n",
    "            elif 0.84 < score < 0.85 and len(sentence1.split()) >= min_length and len(sentence2.split()) >= min_length and abs(len(sentence1) - len(sentence2)) <= 4 * min_length:\n",
    "                ret.append(('IRRELEVANT', sentence1, sentence2, score, []))\n",
    "\n",
    "    #sort by similarity score\n",
    "    ret.sort(key = lambda x: x[3], reverse = True)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanaity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.659658670425415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "diff = extract_similar_from_doc(\n",
    "    'data/full data/t1989-9-canada-russian-federation-bit-1989.xml', \n",
    "    'data/full data/t1990-14-canada-czech-republic-bit-1990.xml',\n",
    "    title2category,\n",
    "    ['Definition']\n",
    ")\n",
    "print(time.time() - start)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out the documents that contain the target category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total treaty is 3309\n",
      "2544\n"
     ]
    }
   ],
   "source": [
    "docs = ['data/full data/' + _ for _ in os.listdir('data/full data') ]\n",
    "print(\"total treaty is \" + str(len(docs)))\n",
    "target_category = ['Public Policy']\n",
    "target_treaty = []\n",
    "\n",
    "\n",
    "\n",
    "for treaty in docs:\n",
    "    try:   \n",
    "        doc1 = ET.parse(treaty)\n",
    "        doc_root1 = doc1.getroot()\n",
    "        doc_body1 = doc_root1[1][2]\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    for article in doc_body1:\n",
    "        try:\n",
    "            # if title2category.get(article.get(\"title\").lower()) in target_category:\n",
    "            if title2category.get(article.get(\"title\").lower()):\n",
    "                target_treaty.append(treaty)\n",
    "                break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "print(len(target_treaty))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [1:42:06<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "#Write all sentences to a excel file\n",
    "wb = xlwt.Workbook()\n",
    "ws = wb.add_sheet('sheet1')\n",
    "\n",
    "#add header\n",
    "ws.write(0, 0, 'label')\n",
    "ws.write(0, 1, 'sentence1')\n",
    "ws.write(0, 2, 'sentence2')\n",
    "ws.write(0, 3, 'similarity')\n",
    "ws.write(0, 4, 'category3')\n",
    "ws.write(0, 5, 'doc1')\n",
    "ws.write(0, 6, 'doc2')\n",
    "\n",
    "\n",
    "row = 1\n",
    "res = []\n",
    "\n",
    "visited = set()\n",
    "\n",
    "for i in tqdm(range(10000), disable = 0):\n",
    "    doc1 = random.choice(target_treaty)\n",
    "    doc1name = doc1.replace('.','/').split('/')[2]\n",
    "\n",
    "    doc2 = random.choice(target_treaty)\n",
    "    doc2name = doc2.replace('.','/').split('/')[2]\n",
    "\n",
    "    if (doc1, doc2) in visited or (doc2, doc1) in visited:\n",
    "        continue\n",
    "    visited.add((doc1, doc2))\n",
    "    visited.add((doc2, doc1))\n",
    "\n",
    "    # try:\n",
    "    diff = extract_similar_from_doc(doc1, doc2, title2category, exclude_category = [])\n",
    "    if any(diff):\n",
    "        res.extend(diff)\n",
    "\n",
    "        for i, (label, sentence1, sentence2, score, category3) in enumerate(diff):\n",
    "            ws.write(row, 0, label)\n",
    "            ws.write(row, 1, sentence1)\n",
    "            ws.write(row, 2, sentence2)\n",
    "            ws.write(row, 3, score)\n",
    "            ws.write(row, 4, str(category3))\n",
    "            ws.write(row, 5, doc1name)\n",
    "            ws.write(row, 6, doc2name)\n",
    "            row += 1\n",
    "\n",
    "wb.save('generated_data/similar_sentences.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6290"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3771"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _ in res if _[0] == 'STYLYSTIC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "888"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for _ in res if _[0] == 'RELEVANT'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('legal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06852cdbcf28824f4856c911260dff4aa3407a086ba977abbb1a931f2b398117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

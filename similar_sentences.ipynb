{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import xlwt\n",
    "\n",
    "from utils import elem2sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and metadata from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import keyword\n",
    "\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "meta_data_df = pd.read_excel('data/category_configuration_09-08-2022_08-08-01.xlsx', sheet_name = 'article_names_matching')\n",
    "title2category = dict(zip(meta_data_df[\"Article Title\"], meta_data_df[\"Category 2\"]))\n",
    "\n",
    "keyword_df = pd.read_excel('data/category_configuration_09-08-2022_08-08-01.xlsx', sheet_name = 'keywords_category_2_mapping')\n",
    "#keyword_df[\"Category\"] = keyword_df[['Category 3', 'Category 2']].apply(\n",
    "#    lambda x: [x[0],x[1]],\n",
    "#    axis=1)\n",
    "keyword2category2 = dict(zip(keyword_df[\"Keyword\"], keyword_df[\"Category 2\"]))\n",
    "keyword2category3 = dict(zip(keyword_df[\"Keyword\"], keyword_df[\"Category 3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for calculate similarity between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_alignment(all_articles1, all_articles2, title2category, sanity_check = True):\n",
    "    '''\n",
    "    Get article alignment between two documents\n",
    "    article_body: xml element\n",
    "    title2category: dictionary of title to category\n",
    "    sanity_check: use similarity score to check if the alignment is correct\n",
    "    return a dictionary of article alignment\n",
    "    '''\n",
    "    alignment_match = {}\n",
    "\n",
    "    #filter out the arrticles that don't have title\n",
    "    all_articles1 = [article for article in all_articles1 if article.get(\"title\") is not None]\n",
    "    all_articles2 = [article for article in all_articles2 if article.get(\"title\") is not None]\n",
    "\n",
    "    #if both articles attribute includes title\n",
    "    #if 'title' in all_articles1[0].attrib and 'title' in all_articles2[0].attrib:\n",
    "    article_title1 = [(article.get('title').lower(), article.get('num')) for article in all_articles1 if article.get('title')]\n",
    "    article_title2 = [(article.get('title').lower(), article.get('num')) for article in all_articles2 if article.get('title')]\n",
    "    for index1, (title1, num1) in enumerate(article_title1):\n",
    "        for index2, (title2, num2) in enumerate(article_title2):\n",
    "            if title2category.get(title1) == title2category.get(title2):\n",
    "                #to make sure that they have a high similarity\n",
    "                if sanity_check and max(util.cos_sim(\n",
    "                    model.encode(''.join(elem2sent(all_articles1[index1]))),\n",
    "                    model.encode(''.join(elem2sent(all_articles2[index2])))\n",
    "                )).item() < 0.7:\n",
    "                    continue\n",
    "                alignment_match[num1] = num2\n",
    "                break\n",
    "                \n",
    "    #title not included in article attri, use sentence similarity instead\n",
    "    # else:\n",
    "    #     for article1 in all_articles1:\n",
    "    #         text1_embd = model.encode(''.join(elem2sent(article1)))\n",
    "    #         text2_list_embd = model.encode([''.join(elem2sent(article2)) for article2 in all_articles2])\n",
    "    #         scores = util.cos_sim(text1_embd, text2_list_embd)\n",
    "    #         if max(scores[0]).item() > 0.7:\n",
    "    #             index = np.argmax(scores[0])\n",
    "    #             alignment_match[article1.get('num')] = all_articles2[index].get('num')\n",
    "\n",
    "    return alignment_match\n",
    "    \n",
    "\n",
    "def extract_similar_sentences_from_article(article1, article2):\n",
    "    '''\n",
    "    article1, article2: xml element\n",
    "    return: list of similar sentences and their category: (sentence1, sentence2, similarity), category\n",
    "    '''\n",
    "    if article1.get('title') and article2.get('title'):\n",
    "        assert title2category.get(article1.get('title').lower()) == title2category.get(article2.get('title').lower())\n",
    "\n",
    "    article1_sents, article2_sents = elem2sent(article1, break_sentence = False), elem2sent(article2, break_sentence = False)\n",
    "    #article1_sents, article2_sents = elem2sent(article1), elem2sent(article2)\n",
    "\n",
    "    #Embed article1 and article2\n",
    "    article1_embd, article2_embd = model.encode(article1_sents), model.encode(article2_sents)\n",
    "\n",
    "    #Get similarity between article1 and article2\n",
    "    scores = util.cos_sim(article1_embd, article2_embd)\n",
    "\n",
    "    visited = set() #to make sure that we don't add the same sentence twice\n",
    "\n",
    "    #filter out the sentence with similarity greater than 0.98, this means they are perfect match and no need to compare\n",
    "    identical = (scores > 0.98).to(torch.int64)\n",
    "    for i, j in identical.nonzero().tolist():\n",
    "        visited.add('row' + str(i))\n",
    "        visited.add('col' + str(j))\n",
    "\n",
    "    #filter out the sentences with similarity between 0.5 and 0.98\n",
    "    mask = (scores > 0.5) & (scores < 0.98)\n",
    "    scores *= mask.to(torch.int64) \n",
    "\n",
    "    #get the index of the sentences with similarity between 0.5 and 0.98\n",
    "    sim_pairs = [(scores[i][j], i, j) for i, j in mask.nonzero().tolist()]\n",
    "    sim_pairs.sort(key = lambda x: x[0]) #sort by similarity score\n",
    "    \n",
    "    ret = []\n",
    "    while sim_pairs:\n",
    "        score, i, j = sim_pairs.pop()\n",
    "        if 'row' + str(i) not in visited and 'col' + str(j) not in visited:\n",
    "            ret.append((article1_sents[i], article2_sents[j], scores[i][j].item()))\n",
    "            visited.add('row' + str(i))\n",
    "            visited.add('col' + str(j))\n",
    "\n",
    "    return ret, title2category.get(article1.get('title').lower())\n",
    "\n",
    "def extract_similar_from_doc(doc1_path, doc2_path, title2category, target_category = ['Fair and Equitable Treatment'], min_length = 5):\n",
    "    '''\n",
    "    doc_path: path to first document\n",
    "    title2category: dictionary of title to category\n",
    "    '''\n",
    "\n",
    "    try:   \n",
    "        doc1, doc2 = ET.parse(doc1_path), ET.parse(doc2_path)\n",
    "        doc_root1, doc_root2 = doc1.getroot(), doc2.getroot()\n",
    "        doc_body1, doc_body2 = doc_root1[1][2], doc_root2[1][2]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e); return []\n",
    "    \n",
    "    #get article alignment between two documents\n",
    "    all_articles1, all_articles2 = doc_body1.findall(\".//div[@type='article']\"), doc_body2.findall(\".//div[@type='article']\")\n",
    "    #alignment_match = get_article_alignment(doc_body1, doc_body2, title2category)\n",
    "    alignment_match = get_article_alignment(all_articles1, all_articles2, title2category)\n",
    "    #print(alignment_match)\n",
    "    keywords = [k for k, v in keyword2category2.items() if v in target_category]\n",
    "\n",
    "    ret = []\n",
    "    for page1, page2 in alignment_match.items():\n",
    "        article1, article2 = doc_body1.find(\".//div[@num='\" + page1 + \"']\"), doc_body2.find(\".//div[@num='\" + page2 + \"']\") \n",
    "        similar_sents, category = extract_similar_sentences_from_article(article1, article2)\n",
    "\n",
    "        if category in target_category:\n",
    "            for (sentence1, sentence2, score) in similar_sents:\n",
    "\n",
    "                s1_cate3 = [keyword2category3.get(key) for key in keywords if key in sentence1.lower()]\n",
    "                s2_cate3 = [keyword2category3.get(key) for key in keywords if key in sentence2.lower()]\n",
    "\n",
    "                if set(s1_cate3).intersection(s2_cate3) :\n",
    "\n",
    "                    ret.append((sentence1, sentence2, score, list(set(s1_cate3).intersection(s2_cate3))[0]))\n",
    "\n",
    "    #filter out the pairs in which both sentences are longer than min_length words and length difference is less than 4 * min_length\n",
    "    #ret = [x for x in ret if len(x[0].split()) > min_length and len(x[1].split()) > min_length and abs(len(x[0].split()) - len(x[1].split())) < 5 * min_length]\n",
    "    ret = [x for x in ret if len(x[0].split()) > min_length and len(x[1].split()) > min_length]\n",
    "    #sort by similarity score\n",
    "    ret.sort(key = lambda x: x[2], reverse = True)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanaity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\73183\\AppData\\Local\\Temp\\ipykernel_33728\\1730355243.py:65: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  for i, j in identical.nonzero().tolist():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.908003091812134\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "diff = extract_similar_from_doc(\n",
    "    'data/full data/t1989-9-canada-russian-federation-bit-1989.xml', \n",
    "    'data/full data/t1990-14-canada-czech-republic-bit-1990.xml',\n",
    "    title2category\n",
    ")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering out the documents that contain the target category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total treaty is 3309\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "docs = ['data/full data/' + _ for _ in os.listdir('data/full data') ]\n",
    "print(\"total treaty is \" + str(len(docs)))\n",
    "target_category = ['Fair and Equitable Treatment']\n",
    "target_treaty = []\n",
    "\n",
    "for treaty in docs:\n",
    "    try:   \n",
    "        doc1 = ET.parse(treaty)\n",
    "        doc_root1 = doc1.getroot()\n",
    "        doc_body1 = doc_root1[1][2]\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    for article in doc_body1:\n",
    "        try:\n",
    "            if title2category.get(article.get(\"title\").lower()) in target_category:\n",
    "                target_treaty.append(treaty)\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(len(target_treaty))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "canada_docs = target_treaty\n",
    "\n",
    "#Write all sentences to a excel file\n",
    "wb = xlwt.Workbook()\n",
    "ws = wb.add_sheet('sheet1')\n",
    "\n",
    "#add header\n",
    "ws.write(0, 0, 'sentence1')\n",
    "ws.write(0, 1, 'sentence2')\n",
    "ws.write(0, 2, 'similarity')\n",
    "ws.write(0, 3, 'doc1')\n",
    "ws.write(0, 4, 'doc2')\n",
    "ws.write(0, 5, 'Subcategory')\n",
    "\n",
    "row = 1\n",
    "\n",
    "visited = set()\n",
    "\n",
    "for i in range(100):\n",
    "    doc1 = random.choice(canada_docs)\n",
    "    doc1name = doc1.replace('.','/').split('/')[2]\n",
    "\n",
    "    doc2 = random.choice(canada_docs)\n",
    "    doc2name = doc2.replace('.','/').split('/')[2]\n",
    "\n",
    "    if (doc1, doc2) in visited or (doc2, doc1) in visited:\n",
    "\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        _ = extract_similar_from_doc(doc1, doc2, title2category, target_category)\n",
    "        for j in range(len(_)):\n",
    "            ws.write(row, 0, _[j][0])\n",
    "            ws.write(row, 1, _[j][1])\n",
    "            ws.write(row, 2, _[j][2])\n",
    "            ws.write(row, 3, doc1name)\n",
    "            ws.write(row, 4, doc2name)\n",
    "            ws.write(row, 5, _[j][3])\n",
    "\n",
    "            row += 1\n",
    "            visited.add((doc1, doc2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(doc1, doc2)\n",
    "        continue\n",
    "\n",
    "wb.save('generated_data/similar_sentences.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set({\"a\", \"b\", \"c\"})\n",
    "\n",
    "\"a\" in a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problematic sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Each Contracting Party undertakes to ensure, in its territory and in its maritime zone, fair and equitable treatment, in accordance with the principles of international law, to investments of investors of the other Party, excluding any unfair or discriminatory measure which might hinder in law or in fact the management, maintenance, enjoyment or liquidation of such investments.',\n",
       "  '1. Each Contracting Party shall accord to investments of investors as of the other contracting party treatment pursuant to the customary International Law including fair and equitable treatment and full protection and security.',\n",
       "  0.8035207986831665,\n",
       "  'Fair and Equitable Treatment')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = 't1991-3-france-united-arab-emirates-bit-1991'\n",
    "doc2 = 't2015-28-haiti-mexico-bit-2015'\n",
    "\n",
    "extract_similar_from_doc('data/full data/' + doc1 + '.xml', 'data/full data/' + doc2 + '.xml', title2category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b763bb86ee93252df19d4bd54cbe9276292bdb09ab6bdcdd7ab663e1bedab76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
